{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc277e6-3209-450e-a252-cbb4497ace3d",
   "metadata": {},
   "source": [
    "# Practice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47620a-0385-41e6-b162-0383272d27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431cf4c9-9241-41ba-a454-949ea0dccc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f18e04-c354-4f33-b033-bc881d12ded3",
   "metadata": {},
   "source": [
    "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "1e4da565-5587-4985-9252-9eee1e7a93c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training data into train and validation subsets (e.g., 80%/20%)\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "0f16c0d1-0e05-4abc-8ecc-4fb9cb7a5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        torch.nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b1dff4f1-09d1-4588-9f07-d581d1803271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(Swish())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(Swish())\n",
    "\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce5af2-d74a-49cb-a0b1-f8d4963c172a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c65dabd5-eeaf-4274-bda5-fc2c25027abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, device, epochs=50, patience=5):\n",
    "    best_val_loss = 0.0\n",
    "    patience_counter = 0\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        per_epoch_train_loss = 0.0\n",
    "        time_per_epoch = time.time()\n",
    "        for data in train_loader:\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            per_epoch_train_loss += loss.item()\n",
    "\n",
    "        avg_per_epoch_train_loss = per_epoch_train_loss / len(train_loader)\n",
    "\n",
    "        per_epoch_val_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in val_loader:\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                per_epoch_val_loss += loss.item()\n",
    "                \n",
    "        avg_per_epoch_val_loss = per_epoch_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_per_epoch_train_loss:.4f}, \" \n",
    "              f\"Val Loss: {avg_per_epoch_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Time Elapsed {time.time() - time_per_epoch:.3f}s\")\n",
    "\n",
    "        if best_val_loss <= avg_per_epoch_val_loss:\n",
    "            patience_counter = 0\n",
    "            best_val_loss = avg_per_epoch_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"\\nTotal Time for Training {end-start:.3f/60}m\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "c40e81b0-3d08-4dea-aee2-69216b18ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs  = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "aa2f9902-974d-4fd6-b6c7-7cc1e4354622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 1.9034, Val Loss: 1.7534, Val Acc: 34.59%, Time Elapsed 23.628s\n",
      "Epoch [2/5], Train Loss: 1.6680, Val Loss: 1.6468, Val Acc: 42.27%, Time Elapsed 26.261s\n",
      "Epoch [3/5], Train Loss: 1.5601, Val Loss: 1.5925, Val Acc: 43.22%, Time Elapsed 25.203s\n",
      "Epoch [4/5], Train Loss: 1.4846, Val Loss: 1.5574, Val Acc: 45.04%, Time Elapsed 47.351s\n",
      "Epoch [5/5], Train Loss: 1.4197, Val Loss: 1.5426, Val Acc: 46.60%, Time Elapsed 51.594s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "cifar10  = CIFAR10()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=0.001)\n",
    "model = train(cifar10, train_loader, val_loader, criterion, optimizer, device=device, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "76d1c3b6-4aa1-40a4-8b61-546071cf4c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 46.24%\n"
     ]
    }
   ],
   "source": [
    "eval(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7691b0a-e2f1-4c0e-b999-11221b368ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
