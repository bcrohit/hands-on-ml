{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc277e6-3209-450e-a252-cbb4497ace3d",
   "metadata": {},
   "source": [
    "# Practice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47620a-0385-41e6-b162-0383272d27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431cf4c9-9241-41ba-a454-949ea0dccc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f18e04-c354-4f33-b033-bc881d12ded3",
   "metadata": {},
   "source": [
    "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "1e4da565-5587-4985-9252-9eee1e7a93c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training data into train and validation subsets (e.g., 80%/20%)\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "0f16c0d1-0e05-4abc-8ecc-4fb9cb7a5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        torch.nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b1dff4f1-09d1-4588-9f07-d581d1803271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(Swish())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(Swish())\n",
    "\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce5af2-d74a-49cb-a0b1-f8d4963c172a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "c65dabd5-eeaf-4274-bda5-fc2c25027abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs, criterion, optimizer, device='cpu'):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        per_epoch_train_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            per_epoch_train_loss += loss.item()\n",
    "\n",
    "        avg_per_epoch_train_loss = per_epoch_train_loss / len(train_loader)\n",
    "\n",
    "        per_epoch_val_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in val_loader:\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                per_epoch_val_loss += loss.item()\n",
    "                \n",
    "        avg_per_epoch_val_loss = per_epoch_val_loss / len(val_loader)\n",
    "        val_accuracy = 100* correct / total\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_per_epoch_train_loss:.4f}, \" \n",
    "              f\"Val Loss: {avg_per_epoch_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "aa2f9902-974d-4fd6-b6c7-7cc1e4354622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 1.8810, Val Loss: 1.7057, Val Acc: 38.55%\n",
      "Epoch [2/5], Train Loss: 1.6413, Val Loss: 1.6306, Val Acc: 43.56%\n",
      "Epoch [3/5], Train Loss: 1.5393, Val Loss: 1.5646, Val Acc: 45.15%\n",
      "Epoch [4/5], Train Loss: 1.4676, Val Loss: 1.5610, Val Acc: 45.16%\n",
      "Epoch [5/5], Train Loss: 1.4088, Val Loss: 1.5284, Val Acc: 47.26%\n"
     ]
    }
   ],
   "source": [
    "cifar10  = CIFAR10()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=0.001)\n",
    "train(cifar10, train_loader, val_loader, 5, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a2889-793c-4dbe-b986-db48f9dbb158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
