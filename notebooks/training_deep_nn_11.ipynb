{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad28e11",
   "metadata": {},
   "source": [
    "# Practice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2919d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f78f23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7317b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, device='cpu', scheduler=None, epochs=50, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        per_epoch_train_loss = 0.0\n",
    "        time_per_epoch = time.time()\n",
    "        for data in train_loader:\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            per_epoch_train_loss += loss.item()\n",
    "\n",
    "        avg_per_epoch_train_loss = per_epoch_train_loss / len(train_loader)\n",
    "        writer.add_scalar(\"Loss/Train\", avg_per_epoch_train_loss, epoch+1)\n",
    "\n",
    "        per_epoch_val_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in val_loader:\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                loss = criterion(outputs, targets)                \n",
    "                per_epoch_val_loss += loss.item()\n",
    "                \n",
    "        avg_per_epoch_val_loss = per_epoch_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        writer.add_scalar(\"Loss/Val\", avg_per_epoch_val_loss, epoch+1)\n",
    "        writer.add_scalar(\"Accuracy/Val\", val_accuracy, epoch+1)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_per_epoch_train_loss:.4f}, \" \n",
    "              f\"Val Loss: {avg_per_epoch_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Time Elapsed {time.time() - time_per_epoch:.3f}s\")\n",
    "\n",
    "        if  avg_per_epoch_val_loss < best_val_loss:\n",
    "            patience_counter = 0\n",
    "            best_val_loss = avg_per_epoch_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"\\nTotal Time for Training {(end-start)/60:.3f}m\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532321f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs  = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed700c1",
   "metadata": {},
   "source": [
    "**A. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function.()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c072277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10V1(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10V1, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(Swish())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(Swish())\n",
    "\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd1fea3",
   "metadata": {},
   "source": [
    "### He Initialization (Kaiming Initialization) in PyTorch\n",
    "\n",
    "He initialization, also called **Kaiming initialization**, is used to initialize weights. According to this, the weight parameters should be sampled from a distribution with:\n",
    "\n",
    "$$\\text{mean} = 0, \\;\\; \\text{variance} = \\frac{2}{\\text{fan\\_in}}\\;\\;\\;i.e.,\\;\\;\\;W \\sim \\mathcal{N} \\left( 0, \\frac{2}{\\text{fan\\_in}} \\right)$$  \n",
    "\n",
    "\n",
    "\n",
    "In PyTorch, there is no direct way to set the variance explicitly to $$(\\frac{2}{\\text{fan\\_in}})$$ using `torch.nn.init.kaiming_normal_()`, since the `nonlinearity` argument only accepts `linear`, `relu`, and `leaky_relu` as arguments.\n",
    "\n",
    "For **Leaky ReLU**, the gain and std is computed as:  \n",
    "\n",
    "$$\\text{gain} = \\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}} \\;\\;\\;\\;\n",
    "\\text{std} = \\sqrt{\\frac{\\text{gain}}{\\sqrt{\\text{fan\\_mode}}}},\\;\\;\\;\n",
    "\\text{variance} = \\frac{\\text{gain}}{\\sqrt{\\text{fan\\_mode}}}$$  \n",
    "\n",
    "\n",
    "To ensure the variance matches $$(\\frac{2}{\\text{fan\\_in}}),\\;\\;\\; we\\;need\\;to\\;set\\;\\;negative\\_slope=0\\;\\;and\\;\\;fan\\_mode=fan\\_in$$ \n",
    "\n",
    "Since `torch.nn.init.kaiming_normal_()` takes a parameter **`a`**, which denotes `negative_slope`, setting `a=0` makes sure that He initialization is applied correctly.  \n",
    "\n",
    "\n",
    "**I have also implemented `nn.SiLU` (Swish activation function) manually. I do not know why I did this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359a1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layer, nonlinearity='leaky_relu'):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        if nonlinearity=='leaky_relu': # param a is only used with leaky_relu\n",
    "            torch.nn.init.kaiming_normal_(layer.weight, mode='fan_in', a=0, nonlinearity=nonlinearity)\n",
    "        else:\n",
    "            torch.nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity=nonlinearity)\n",
    "\n",
    "        torch.nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d7615",
   "metadata": {},
   "source": [
    "**B. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0302cb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:01<00:00, 88724437.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training data into train and validation subsets (e.g., 80%/20%)\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065dd4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 1.8834, Val Loss: 2.0342, Val Acc: 28.66%, Time Elapsed 12.412s\n",
      "Epoch [2/100], Train Loss: 1.6588, Val Loss: 1.7618, Val Acc: 37.16%, Time Elapsed 12.223s\n",
      "Epoch [3/100], Train Loss: 1.5673, Val Loss: 1.7157, Val Acc: 39.76%, Time Elapsed 12.256s\n",
      "Epoch [4/100], Train Loss: 1.4956, Val Loss: 1.6138, Val Acc: 42.76%, Time Elapsed 12.233s\n",
      "Epoch [5/100], Train Loss: 1.4386, Val Loss: 1.5578, Val Acc: 45.02%, Time Elapsed 12.306s\n",
      "Epoch [6/100], Train Loss: 1.3937, Val Loss: 1.5904, Val Acc: 42.98%, Time Elapsed 12.227s\n",
      "Epoch [7/100], Train Loss: 1.3549, Val Loss: 1.5189, Val Acc: 46.06%, Time Elapsed 12.202s\n",
      "Epoch [8/100], Train Loss: 1.3138, Val Loss: 1.5711, Val Acc: 44.90%, Time Elapsed 12.313s\n",
      "Epoch [9/100], Train Loss: 1.2747, Val Loss: 1.5244, Val Acc: 46.40%, Time Elapsed 12.340s\n",
      "Epoch [10/100], Train Loss: 1.2431, Val Loss: 1.5137, Val Acc: 47.56%, Time Elapsed 12.246s\n",
      "Epoch [11/100], Train Loss: 1.2127, Val Loss: 1.5083, Val Acc: 48.00%, Time Elapsed 12.221s\n",
      "Epoch [12/100], Train Loss: 1.1809, Val Loss: 1.5486, Val Acc: 47.52%, Time Elapsed 12.223s\n",
      "Epoch [13/100], Train Loss: 1.1499, Val Loss: 1.4983, Val Acc: 48.08%, Time Elapsed 12.220s\n",
      "Epoch [14/100], Train Loss: 1.1261, Val Loss: 1.5346, Val Acc: 48.68%, Time Elapsed 12.256s\n",
      "Epoch [15/100], Train Loss: 1.0978, Val Loss: 1.5439, Val Acc: 48.84%, Time Elapsed 12.275s\n",
      "Epoch [16/100], Train Loss: 1.0739, Val Loss: 1.5817, Val Acc: 47.54%, Time Elapsed 12.177s\n",
      "Epoch [17/100], Train Loss: 1.0562, Val Loss: 1.5199, Val Acc: 48.86%, Time Elapsed 12.187s\n",
      "Epoch [18/100], Train Loss: 1.0294, Val Loss: 1.5684, Val Acc: 48.76%, Time Elapsed 12.148s\n",
      "Early stopping triggered.\n",
      "\n",
      "Total Time for Training 3.675m\n",
      "Test Accuracy: 50.09%\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "cifar10  = CIFAR10V1().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=0.001)\n",
    "model_v1 = train(cifar10, train_loader, val_loader, criterion, optimizer, device=device, epochs=epochs)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "# Model Accuracy\n",
    "eval(model_v1, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731062c",
   "metadata": {},
   "source": [
    "**C. Now try adding batch normalization and compare the learning curves: is it converging faster than before? Does it produce a better model? How does it affect training speed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f60b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10V2(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10V2, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(nn.BatchNorm1d(output_neurons))\n",
    "        layers.append(Swish())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.BatchNorm1d(output_neurons))\n",
    "            layers.append(Swish())\n",
    "\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82d0d380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 1.9657, Val Loss: 1.8295, Val Acc: 34.22%, Time Elapsed 14.770s\n",
      "Epoch [2/100], Train Loss: 1.6850, Val Loss: 1.7463, Val Acc: 37.30%, Time Elapsed 14.673s\n",
      "Epoch [3/100], Train Loss: 1.5941, Val Loss: 1.6492, Val Acc: 41.38%, Time Elapsed 14.746s\n",
      "Epoch [4/100], Train Loss: 1.5122, Val Loss: 1.7223, Val Acc: 39.36%, Time Elapsed 14.837s\n",
      "Epoch [5/100], Train Loss: 1.4587, Val Loss: 1.6158, Val Acc: 43.58%, Time Elapsed 14.752s\n",
      "Epoch [6/100], Train Loss: 1.4127, Val Loss: 1.5324, Val Acc: 45.90%, Time Elapsed 14.757s\n",
      "Epoch [7/100], Train Loss: 1.3632, Val Loss: 1.5499, Val Acc: 45.28%, Time Elapsed 14.706s\n",
      "Epoch [8/100], Train Loss: 1.3304, Val Loss: 1.4972, Val Acc: 47.10%, Time Elapsed 14.718s\n",
      "Epoch [9/100], Train Loss: 1.2928, Val Loss: 1.6656, Val Acc: 42.26%, Time Elapsed 14.676s\n",
      "Epoch [10/100], Train Loss: 1.2834, Val Loss: 1.5026, Val Acc: 47.88%, Time Elapsed 14.738s\n",
      "Epoch [11/100], Train Loss: 1.2338, Val Loss: 1.4563, Val Acc: 49.58%, Time Elapsed 14.737s\n",
      "Epoch [12/100], Train Loss: 1.1959, Val Loss: 1.4500, Val Acc: 50.00%, Time Elapsed 14.795s\n",
      "Epoch [13/100], Train Loss: 1.1659, Val Loss: 1.4820, Val Acc: 49.30%, Time Elapsed 14.723s\n",
      "Epoch [14/100], Train Loss: 1.1392, Val Loss: 1.4943, Val Acc: 49.78%, Time Elapsed 14.723s\n",
      "Epoch [15/100], Train Loss: 1.1172, Val Loss: 1.5166, Val Acc: 49.40%, Time Elapsed 14.669s\n",
      "Epoch [16/100], Train Loss: 1.0986, Val Loss: 1.5103, Val Acc: 50.38%, Time Elapsed 14.626s\n",
      "Epoch [17/100], Train Loss: 1.0691, Val Loss: 1.5011, Val Acc: 48.72%, Time Elapsed 14.616s\n",
      "Early stopping triggered.\n",
      "\n",
      "Total Time for Training 4.171m\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "cifar10  = CIFAR10V2().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=0.001)\n",
    "model_v2 = train(cifar10, train_loader, val_loader, criterion, optimizer, device=device, epochs=epochs)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f8f58b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 51.26%\n"
     ]
    }
   ],
   "source": [
    "eval(model_v2, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e33e385",
   "metadata": {},
   "source": [
    "***Is the model converging faster than before? - Yes, to some extent (only by 2-3 epochs). Maybe experimenting with different learning rates can yield much faster convergence.***\n",
    "\n",
    "***Does BN produce a better model?  - Yes, adding batch normalization boosted the accuracy by ~2%***\n",
    "\n",
    "***How does BN affect training speed? - Yes and No, previously it took ~12s for each epoch, but this one's taking ~15s. But, can be said that total training time did reduce since the model also acheived a better accuracy.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e04fc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06738a2c",
   "metadata": {},
   "source": [
    "**D. Try replacing batch normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).**\n",
    "\n",
    "*Note that input features were already normalized while downloading data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b59cebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10V3(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10V3, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(nn.SELU())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.SELU())\n",
    "\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init_lecun)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d797acc",
   "metadata": {},
   "source": [
    "**LeCun Normal Initialization** is called **Xavier Normal Initialization** (`torch.nn.init.xavier_normal_`).  \n",
    "But, LeCun Initialization is not directly supported in PyTorch, so here also we need to do some maths.  \n",
    "\n",
    "The standard deviation for `torch.nn.init.xavier_normal_()` is given in the [documentation](https://shorturl.at/WxECS).  \n",
    "\n",
    "\n",
    "LeCun Normal Initialization uses a standard deviation of:  \n",
    "\n",
    "$$\\text{std} = \\sqrt{\\frac{1}{\\text{fan\\_in}}}$$  \n",
    "\n",
    "To match this, we need to set the **gain** as follows:  \n",
    "\n",
    "$$\\text{gain} = \\sqrt{\\frac{\\text{fan\\_in} + \\text{fan\\_out}}{\\text{fan\\_in}}}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fdeb10aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_lecun(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        gain = np.sqrt(sum(layer.weight.shape) / layer.weight.shape[1])\n",
    "        torch.nn.init.xavier_normal_(layer.weight, gain=gain)\n",
    "        torch.nn.init.constant_(layer.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "667d7ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 2.2196, Val Loss: 2.0358, Val Acc: 24.50%, Time Elapsed 11.658s\n",
      "Epoch [2/100], Train Loss: 1.7954, Val Loss: 1.9671, Val Acc: 32.20%, Time Elapsed 11.678s\n",
      "Epoch [3/100], Train Loss: 1.6761, Val Loss: 1.8195, Val Acc: 35.60%, Time Elapsed 11.648s\n",
      "Epoch [4/100], Train Loss: 1.6029, Val Loss: 1.7836, Val Acc: 36.92%, Time Elapsed 11.693s\n",
      "Epoch [5/100], Train Loss: 1.5488, Val Loss: 1.6671, Val Acc: 39.30%, Time Elapsed 11.745s\n",
      "Epoch [6/100], Train Loss: 1.4995, Val Loss: 1.7507, Val Acc: 40.52%, Time Elapsed 11.770s\n",
      "Epoch [7/100], Train Loss: 1.4641, Val Loss: 1.9306, Val Acc: 33.80%, Time Elapsed 11.700s\n",
      "Epoch [8/100], Train Loss: 1.4162, Val Loss: 1.7136, Val Acc: 40.90%, Time Elapsed 11.664s\n",
      "Epoch [9/100], Train Loss: 1.3872, Val Loss: 1.7040, Val Acc: 41.82%, Time Elapsed 11.731s\n",
      "Epoch [10/100], Train Loss: 1.3541, Val Loss: 1.5268, Val Acc: 46.22%, Time Elapsed 11.685s\n",
      "Epoch [11/100], Train Loss: 1.3193, Val Loss: 1.5018, Val Acc: 48.04%, Time Elapsed 11.648s\n",
      "Epoch [12/100], Train Loss: 1.2849, Val Loss: 1.5130, Val Acc: 46.88%, Time Elapsed 11.687s\n",
      "Epoch [13/100], Train Loss: 1.2504, Val Loss: 1.6056, Val Acc: 45.46%, Time Elapsed 11.650s\n",
      "Epoch [14/100], Train Loss: 1.2164, Val Loss: 1.5593, Val Acc: 47.50%, Time Elapsed 11.719s\n",
      "Epoch [15/100], Train Loss: 1.2044, Val Loss: 1.5267, Val Acc: 47.80%, Time Elapsed 11.730s\n",
      "Epoch [16/100], Train Loss: 1.1704, Val Loss: 1.5948, Val Acc: 47.46%, Time Elapsed 11.836s\n",
      "Early stopping triggered.\n",
      "\n",
      "Total Time for Training 3.121m\n",
      "Test Accuracy: 48.25%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "cifar10  = CIFAR10V3().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=0.001)\n",
    "model_v3 = train(cifar10, train_loader, val_loader, criterion, optimizer, device=device, epochs=epochs)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "eval(model_v3, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160773c6",
   "metadata": {},
   "source": [
    "***Though training time reduced, accuracy is worse then the first model. But lowest val loss was reached in just 11 epochs.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f375a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "176ed0f5",
   "metadata": {},
   "source": [
    "**E. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC dropout.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07465aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10V4(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10V4, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(nn.SELU())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.SELU())\n",
    "\n",
    "        layers.append(nn.AlphaDropout(p=0.1))\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init_lecun)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbb7bd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 2.1604, Val Loss: 1.9445, Val Acc: 28.74%, Time Elapsed 11.799s\n",
      "Epoch [2/100], Train Loss: 1.7827, Val Loss: 1.9209, Val Acc: 31.60%, Time Elapsed 11.857s\n",
      "Epoch [3/100], Train Loss: 1.6765, Val Loss: 2.0588, Val Acc: 28.84%, Time Elapsed 11.808s\n",
      "Epoch [4/100], Train Loss: 1.6118, Val Loss: 1.6568, Val Acc: 39.78%, Time Elapsed 11.945s\n",
      "Epoch [5/100], Train Loss: 1.5567, Val Loss: 1.7301, Val Acc: 41.60%, Time Elapsed 11.801s\n",
      "Epoch [6/100], Train Loss: 1.5161, Val Loss: 1.6173, Val Acc: 42.48%, Time Elapsed 11.767s\n",
      "Epoch [7/100], Train Loss: 1.4743, Val Loss: 1.6345, Val Acc: 43.50%, Time Elapsed 11.703s\n",
      "Epoch [8/100], Train Loss: 1.4420, Val Loss: 1.5671, Val Acc: 45.00%, Time Elapsed 11.779s\n",
      "Epoch [9/100], Train Loss: 1.4074, Val Loss: 1.6784, Val Acc: 44.80%, Time Elapsed 11.763s\n",
      "Epoch [10/100], Train Loss: 1.3755, Val Loss: 1.6277, Val Acc: 43.80%, Time Elapsed 11.893s\n",
      "Epoch [11/100], Train Loss: 1.3439, Val Loss: 1.5694, Val Acc: 46.58%, Time Elapsed 11.799s\n",
      "Epoch [12/100], Train Loss: 1.3143, Val Loss: 1.5648, Val Acc: 47.00%, Time Elapsed 11.831s\n",
      "Epoch [13/100], Train Loss: 1.2819, Val Loss: 1.5445, Val Acc: 47.32%, Time Elapsed 11.798s\n",
      "Epoch [14/100], Train Loss: 1.2531, Val Loss: 1.5585, Val Acc: 46.56%, Time Elapsed 11.867s\n",
      "Epoch [15/100], Train Loss: 1.2318, Val Loss: 1.5283, Val Acc: 46.48%, Time Elapsed 11.728s\n",
      "Epoch [16/100], Train Loss: 1.2037, Val Loss: 1.6114, Val Acc: 45.66%, Time Elapsed 11.761s\n",
      "Epoch [17/100], Train Loss: 1.1724, Val Loss: 1.5532, Val Acc: 48.16%, Time Elapsed 11.733s\n",
      "Epoch [18/100], Train Loss: 1.1460, Val Loss: 1.5971, Val Acc: 46.86%, Time Elapsed 11.815s\n",
      "Epoch [19/100], Train Loss: 1.1260, Val Loss: 1.6013, Val Acc: 48.38%, Time Elapsed 11.786s\n",
      "Epoch [20/100], Train Loss: 1.0910, Val Loss: 1.5963, Val Acc: 47.06%, Time Elapsed 11.855s\n",
      "Early stopping triggered.\n",
      "\n",
      "Total Time for Training 3.935m\n",
      "Test Accuracy: 47.29%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "cifar10  = CIFAR10V4().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=0.001)\n",
    "model_v4 = train(cifar10, train_loader, val_loader, criterion, optimizer, device=device, epochs=epochs)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "eval(model_v4, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2941cae",
   "metadata": {},
   "source": [
    "***Not as good as the previous one both in terms of accuracy and in terms of time taken***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e9c23b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(nn.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs).train(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732b25b",
   "metadata": {},
   "source": [
    "Note that I could have directly used MCAlphaDropout in the model creation itself, but, whatif I am using an already trained model and want to implement MCAlphaDropout. So, it makes sense to create an identical model only with MCAlphaDropout instead of AlphaDropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7181b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "mc_model = nn.Sequential(*[\n",
    "    MCAlphaDropout(layer.p)\n",
    "    if isinstance(layer, nn.AlphaDropout)\n",
    "    else layer\n",
    "    for layer in model_v4.net\n",
    "]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "359287ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = nn.Flatten()\n",
    "\n",
    "def mc_dropout_predictions(model, test_loader, device='cpu'):\n",
    "    model.train(True) # important\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_probas = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs  = inputs.to(device)\n",
    "        inputs = flatten(inputs)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.to('cpu').detach().numpy()\n",
    "            y_probas.append(outputs)\n",
    "            \n",
    "    return np.vstack(y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ada338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training is set to true, each time model predicts a different output even for the \n",
    "# same inputs, since Dropout layer randomly drops out some neurons with probability p.\n",
    "# Here we get predictions for the test set `mc_iterations` times and then stack it \n",
    "# -> [mc_iterations, 10000, 10] -> 10000 instances and 10 scores for each class.\n",
    "# Then we average over dim=0 and get the prediction scores\n",
    "\n",
    "# mc_model = mc_model.to(device)\n",
    "mc_iterations = 100\n",
    "y_probas = np.stack([mc_dropout_predictions(mc_model, test_loader, device=device) for i in range(mc_iterations)])\n",
    "y_scores = y_probas.mean(axis=0)\n",
    "y_scores = torch.tensor(y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1691af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "targets = np.hstack([targets for _, targets in test_loader])\n",
    "targets = torch.tensor(targets)\n",
    "\n",
    "_, predicted = torch.max(y_scores, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f53adcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 47.29%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy without retraining\n",
    "\n",
    "correct = 0\n",
    "total = targets.size(0)\n",
    "correct += (predicted == targets).sum().item()\n",
    "        \n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9ba87",
   "metadata": {},
   "source": [
    "***Exactly same? This should not happen. Something's off, have to investigate.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd1f790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07a90d54",
   "metadata": {},
   "source": [
    "**F. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15a3dd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 1.5190, Val Loss: 1.7086, Val Acc: 41.98%, Time Elapsed 11.845s\n",
      "Epoch [2/100], Train Loss: 1.4492, Val Loss: 1.6635, Val Acc: 41.02%, Time Elapsed 11.823s\n",
      "Epoch [3/100], Train Loss: 1.4359, Val Loss: 2.1139, Val Acc: 27.62%, Time Elapsed 11.887s\n",
      "Epoch [4/100], Train Loss: 77.3060, Val Loss: 2.4306, Val Acc: 9.96%, Time Elapsed 11.813s\n",
      "Epoch [5/100], Train Loss: 2.3214, Val Loss: 2.3948, Val Acc: 9.96%, Time Elapsed 11.828s\n",
      "Epoch [6/100], Train Loss: 2.3258, Val Loss: 2.3671, Val Acc: 11.42%, Time Elapsed 11.841s\n",
      "Epoch [7/100], Train Loss: 2.3344, Val Loss: 2.4216, Val Acc: 9.96%, Time Elapsed 11.870s\n",
      "Early stopping triggered.\n",
      "\n",
      "Total Time for Training 1.382m\n",
      "Test Accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=3e-2)\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.05, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "model = train(model_v4, train_loader, val_loader, criterion, optimizer, device=device, scheduler=scheduler, epochs=epochs)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "eval(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9292cf49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c935e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
