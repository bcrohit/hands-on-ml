{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc277e6-3209-450e-a252-cbb4497ace3d",
   "metadata": {},
   "source": [
    "# Practice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d47620a-0385-41e6-b162-0383272d27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431cf4c9-9241-41ba-a454-949ea0dccc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea22f7f-183c-4b58-af61-fdf99467d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, device='cpu', scheduler=None, epochs=50, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        per_epoch_train_loss = 0.0\n",
    "        time_per_epoch = time.time()\n",
    "        for data in train_loader:\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            per_epoch_train_loss += loss.item()\n",
    "\n",
    "        avg_per_epoch_train_loss = per_epoch_train_loss / len(train_loader)\n",
    "        writer.add_scalar(\"Loss/Train\", avg_per_epoch_train_loss, epoch+1)\n",
    "\n",
    "        per_epoch_val_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in val_loader:\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                loss = criterion(outputs, targets)                \n",
    "                per_epoch_val_loss += loss.item()\n",
    "                \n",
    "        avg_per_epoch_val_loss = per_epoch_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        writer.add_scalar(\"Loss/Val\", avg_per_epoch_val_loss, epoch+1)\n",
    "        writer.add_scalar(\"Accuracy/Val\", val_accuracy, epoch+1)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_per_epoch_train_loss:.4f}, \" \n",
    "              f\"Val Loss: {avg_per_epoch_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Time Elapsed {time.time() - time_per_epoch:.3f}s\")\n",
    "\n",
    "        if  avg_per_epoch_val_loss < best_val_loss:\n",
    "            patience_counter = 0\n",
    "            best_val_loss = avg_per_epoch_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"\\nTotal Time for Training {(end-start)/60:.3f}m\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9121e8-c373-4930-9107-51dfc4554ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs  = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f18e04-c354-4f33-b033-bc881d12ded3",
   "metadata": {},
   "source": [
    "**A. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function.()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "29d8d5b8-0abc-4e30-8074-35fbe274a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10V1(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10V1, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(Swish())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(Swish())\n",
    "\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efc45a-92dc-415b-89ad-58e46f67ee7e",
   "metadata": {},
   "source": [
    "### He Initialization (Kaiming Initialization) in PyTorch\n",
    "\n",
    "He initialization, also called **Kaiming initialization**, is used to initialize weights. According to this, the weight parameters should be sampled from a distribution with:\n",
    "\n",
    "$$\\text{mean} = 0, \\;\\; \\text{variance} = \\frac{2}{\\text{fan\\_in}}\\;\\;\\;i.e.,\\;\\;\\;W \\sim \\mathcal{N} \\left( 0, \\frac{2}{\\text{fan\\_in}} \\right)$$  \n",
    "\n",
    "\n",
    "\n",
    "In PyTorch, there is no direct way to set the variance explicitly to $$(\\frac{2}{\\text{fan\\_in}})$$ using `torch.nn.init.kaiming_normal_()`, since the `nonlinearity` argument only accepts `linear`, `relu`, and `leaky_relu` as arguments.\n",
    "\n",
    "For **Leaky ReLU**, the gain and std is computed as:  \n",
    "\n",
    "$$\\text{gain} = \\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}} \\;\\;\\;\\;\n",
    "\\text{std} = \\sqrt{\\frac{\\text{gain}}{\\sqrt{\\text{fan\\_mode}}}},\\;\\;\\;\n",
    "\\text{variance} = \\frac{\\text{gain}}{\\sqrt{\\text{fan\\_mode}}}$$  \n",
    "\n",
    "\n",
    "To ensure the variance matches $$(\\frac{2}{\\text{fan\\_in}}),\\;\\;\\; we\\;need\\;to\\;set\\;\\;negative\\_slope=0\\;\\;and\\;\\;fan\\_mode=fan\\_in$$ \n",
    "\n",
    "Since `torch.nn.init.kaiming_normal_()` takes a parameter **`a`**, which denotes `negative_slope`, setting `a=0` makes sure that He initialization is applied correctly.  \n",
    "\n",
    "\n",
    "**I have also implemented `nn.SiLU` (Swish activation function) manually. I do not know why I did this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "fb2b17f0-70df-4e6f-ba7c-4d1935f68013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layer, nonlinearity='leaky_relu'):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        if nonlinearity=='leaky_relu': # param a is only used with leaky_relu\n",
    "            torch.nn.init.kaiming_normal_(layer.weight, mode='fan_in', a=0, nonlinearity=nonlinearity)\n",
    "        else:\n",
    "            torch.nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity=nonlinearity)\n",
    "\n",
    "        torch.nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7912f-92c5-433b-9e02-fd0506681ec5",
   "metadata": {},
   "source": [
    "**B. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4da565-5587-4985-9252-9eee1e7a93c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training data into train and validation subsets (e.g., 80%/20%)\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "aa2f9902-974d-4fd6-b6c7-7cc1e4354622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "cifar10  = CIFAR10V1()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=0.001)\n",
    "model_v1 = train(cifar10, train_loader, val_loader, criterion, optimizer, device=device, epochs=epochs)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "# Model Accuracy\n",
    "eval(model_v1, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb8888-381f-4ff8-ba6b-d65c15184c26",
   "metadata": {},
   "source": [
    "**C. Now try adding batch normalization and compare the learning curves: is it converging faster than before? Does it produce a better model? How does it affect training speed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "b6b92185-8faf-41a1-90cd-4e294ebe91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10V2(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10V2, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(nn.BatchNorm1d(output_neurons))\n",
    "        layers.append(Swish())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.BatchNorm1d(output_neurons))\n",
    "            layers.append(Swish())\n",
    "\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "949d346c-c56a-439a-82e7-edda2468d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 2.0241, Val Loss: 1.8095, Val Acc: 33.79%, Time Elapsed 61.484s\n",
      "Epoch [2/5], Train Loss: 1.7113, Val Loss: 1.6768, Val Acc: 40.24%, Time Elapsed 61.726s\n",
      "Epoch [3/5], Train Loss: 1.6001, Val Loss: 1.6090, Val Acc: 43.22%, Time Elapsed 60.703s\n",
      "Epoch [4/5], Train Loss: 1.5270, Val Loss: 1.5700, Val Acc: 44.20%, Time Elapsed 62.025s\n",
      "Epoch [5/5], Train Loss: 1.4740, Val Loss: 1.5388, Val Acc: 45.88%, Time Elapsed 61.364s\n",
      "\n",
      "Total Time for Training 5.122m\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "cifar10  = CIFAR10V2()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=0.001)\n",
    "model_v2 = train(cifar10, train_loader, val_loader, criterion, optimizer, device=device, epochs=epochs)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292dde65-08f4-4876-b148-0a48668e64e5",
   "metadata": {},
   "source": [
    "**D. Try replacing batch normalization with SELU, and make the necessary adjustments to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).**\n",
    "\n",
    "*Note that input features were already normalized while downloading data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9c815396-c5d2-452f-bb9b-4562d1926e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10V3(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10V3, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(nn.SELU())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.SELU())\n",
    "\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init_lecun)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564eda9-31cd-4aee-abf9-6453bebe1d0a",
   "metadata": {},
   "source": [
    "**LeCun Normal Initialization** is called **Xavier Normal Initialization** (`torch.nn.init.xavier_normal_`).  \n",
    "But, LeCun Initialization is not directly supported in PyTorch, so here also we need to do some maths.  \n",
    "\n",
    "The standard deviation for `torch.nn.init.xavier_normal_()` is given in the [documentation](https://shorturl.at/WxECS).  \n",
    "\n",
    "\n",
    "LeCun Normal Initialization uses a standard deviation of:  \n",
    "\n",
    "$$\\text{std} = \\sqrt{\\frac{1}{\\text{fan\\_in}}}$$  \n",
    "\n",
    "To match this, we need to set the **gain** as follows:  \n",
    "\n",
    "$$\\text{gain} = \\sqrt{\\frac{\\text{fan\\_in} + \\text{fan\\_out}}{\\text{fan\\_in}}}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "f49902f5-9e6f-4289-815f-589ae609ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_lecun(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        gain = np.sqrt(sum(layer.weight.shape) / layer.weight.shape[1])\n",
    "        torch.nn.init.xavier_normal_(layer.weight, gain=gain)\n",
    "        torch.nn.init.constant_(layer.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d78e1371-2be1-4646-802f-a5fc8c9cdf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 2.1656, Val Loss: 2.0423, Val Acc: 28.54%, Time Elapsed 22.917s\n",
      "Epoch [2/5], Train Loss: 1.7683, Val Loss: 2.1380, Val Acc: 30.68%, Time Elapsed 23.921s\n",
      "Epoch [3/5], Train Loss: 1.6571, Val Loss: 1.7635, Val Acc: 39.64%, Time Elapsed 26.930s\n",
      "Epoch [4/5], Train Loss: 1.5897, Val Loss: 2.0045, Val Acc: 32.44%, Time Elapsed 52.490s\n",
      "Epoch [5/5], Train Loss: 1.5328, Val Loss: 1.7638, Val Acc: 39.46%, Time Elapsed 52.903s\n",
      "\n",
      "Total Time for Training 2.988m\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "cifar10  = CIFAR10V3()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=0.001)\n",
    "model_v3 = train(cifar10, train_loader, val_loader, criterion, optimizer, device=device, epochs=epochs)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "eval(model_v3, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45716ca8-09e5-4d2e-9fa5-736cb870c789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f5e1261-a96c-42fc-b6e2-df33fa3184f7",
   "metadata": {},
   "source": [
    "**E. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC dropout.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c112e301-2678-4f1d-8ee2-8e45554efd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10V4(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10V4, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(nn.SELU())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.SELU())\n",
    "\n",
    "        layers.append(nn.AlphaDropout(p=0.1))\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init_lecun)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "2e6fb0aa-8128-456f-af6b-f64a6e0d28cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 1.9838, Val Loss: 2.0739, Val Acc: 22.34%, Time Elapsed 24.830s\n",
      "Epoch [2/5], Train Loss: 1.7948, Val Loss: 2.3868, Val Acc: 16.72%, Time Elapsed 30.773s\n",
      "Epoch [3/5], Train Loss: 1.6686, Val Loss: 1.7714, Val Acc: 33.92%, Time Elapsed 29.112s\n",
      "Epoch [4/5], Train Loss: 1.5122, Val Loss: 1.5469, Val Acc: 43.60%, Time Elapsed 28.316s\n",
      "Epoch [5/5], Train Loss: 1.3736, Val Loss: 1.4552, Val Acc: 47.16%, Time Elapsed 27.857s\n",
      "\n",
      "Total Time for Training 2.348m\n",
      "Test Accuracy: 50.15%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "cifar10  = CIFAR10V4()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=2e-1)\n",
    "model_v4 = train(cifar10, train_loader, val_loader, criterion, optimizer, device=device, epochs=epochs)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "eval(model_v4, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f11f0ccb-7980-4680-99cb-99f6c443f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(nn.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs).train(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324c4d8-de1a-429e-ad64-c54896f7b16c",
   "metadata": {},
   "source": [
    "Note that I could have directly used MCAlphaDropout in the model creation itself, but, whatif I am using an already trained model and want to implement MCAlphaDropout. So, it makes sense to create an identical model only with MCAlphaDropout instead of AlphaDropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "896ed043-5dca-4879-be9f-930d8f924608",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = nn.Sequential(*[\n",
    "    MCAlphaDropout(layer.p)\n",
    "    if isinstance(layer, nn.AlphaDropout)\n",
    "    else layer\n",
    "    for layer in model_v4.net\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0932e69f-63f1-443a-9484-7f9c750a37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predictions(model, test_loader, device='cpu'):\n",
    "    model.train(True) # important\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_probas = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs  = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.detach().numpy()\n",
    "            y_probas.append(outputs)\n",
    "            \n",
    "    return np.vstack(y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "d6dacf30-1375-4eb2-857b-d5c9f6f44043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training is set to true, each time model predicts a different output even for the \n",
    "# same inputs, since Dropout layer randomly drops out some neurons with probability p.\n",
    "# Here we get predictions for the test set `mc_iterations` times and then stack it \n",
    "# -> [mc_iterations, 10000, 10] -> 10000 instances and 10 scores for each class.\n",
    "# Then we average over dim=0 and get the prediction scores\n",
    "\n",
    "mc_iterations = 100\n",
    "y_probas = np.stack([mc_dropout_predictions(mc_model, test_loader) for i in range(mc_iterations)])\n",
    "y_scores = y_probas.mean(axis=0)\n",
    "y_scores = torch.tensor(y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "ecac0564-6516-47ff-b68a-d1d7fd22abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "targets = np.hstack([targets for _, targets in test_loader])\n",
    "targets = torch.tensor(targets)\n",
    "\n",
    "_, predicted = torch.max(y_scores, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c46f81a0-c0fb-4c62-929b-9e5795bab758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 10.24%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy without retraining\n",
    "\n",
    "correct = 0\n",
    "total = targets.size(0)\n",
    "correct += (predicted == targets).sum().item()\n",
    "        \n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116cc75-b5a0-4e80-bbfb-e0da78d3597a",
   "metadata": {},
   "source": [
    "**F. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f56a77-8760-4e99-9077-57b99c6a5c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=3e-2)\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.05, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "model = train(model_v4, train_loader, val_loader, criterion, optimizer, device=device, scheduler=scheduler, epochs=5)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "eval(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ee331-06ef-40c5-8329-01edf5eb53b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a4903-62a6-45bc-b98c-2dbe456eedb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
