{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc277e6-3209-450e-a252-cbb4497ace3d",
   "metadata": {},
   "source": [
    "# Practice training a deep neural network on the CIFAR10 image dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d47620a-0385-41e6-b162-0383272d27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431cf4c9-9241-41ba-a454-949ea0dccc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f18e04-c354-4f33-b033-bc881d12ded3",
   "metadata": {},
   "source": [
    "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1e4da565-5587-4985-9252-9eee1e7a93c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]\n",
    ")\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training data into train and validation subsets (e.g., 80%/20%)\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0f16c0d1-0e05-4abc-8ecc-4fb9cb7a5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layer, nonlinearity='leaky_relu'):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity=nonlinearity)\n",
    "        torch.nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "28bdee99-fe44-4afc-9f1f-4e8ee35e7b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxavier_normal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mgain\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mgenerator\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Fill the input `Tensor` with values using a Xavier normal distribution.\n",
       "\n",
       "The method is described in `Understanding the difficulty of training deep feedforward\n",
       "neural networks` - Glorot, X. & Bengio, Y. (2010). The resulting tensor\n",
       "will have values sampled from :math:`\\mathcal{N}(0, \\text{std}^2)` where\n",
       "\n",
       ".. math::\n",
       "    \\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}}\n",
       "\n",
       "Also known as Glorot initialization.\n",
       "\n",
       "Args:\n",
       "    tensor: an n-dimensional `torch.Tensor`\n",
       "    gain: an optional scaling factor\n",
       "    generator: the torch Generator to sample from (default: None)\n",
       "\n",
       "Examples:\n",
       "    >>> w = torch.empty(3, 5)\n",
       "    >>> nn.init.xavier_normal_(w)\n",
       "\n",
       "Note:\n",
       "    Be aware that ``fan_in`` and ``fan_out`` are calculated assuming\n",
       "    that the weight matrix is used in a transposed manner,\n",
       "    (i.e., ``x @ w.T`` in ``Linear`` layers, where ``w.shape = [fan_out, fan_in]``).\n",
       "    This is important for correct initialization.\n",
       "    If you plan to use ``x @ w``, where ``w.shape = [fan_in, fan_out]``,\n",
       "    pass in a transposed weight matrix, i.e. ``nn.init.xavier_normal_(w.T, ...)``.\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\work\\venv\\lib\\site-packages\\torch\\nn\\init.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.init.xavier_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1dff4f1-09d1-4588-9f07-d581d1803271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(Swish())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(Swish())\n",
    "\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65dabd5-eeaf-4274-bda5-fc2c25027abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, device, epochs=50, patience=5, standardization=None, standardize=False):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        per_epoch_train_loss = 0.0\n",
    "        time_per_epoch = time.time()\n",
    "        for data in train_loader:\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if standardize:\n",
    "                inputs = standardization(inputs)\n",
    "                \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            per_epoch_train_loss += loss.item()\n",
    "\n",
    "        avg_per_epoch_train_loss = per_epoch_train_loss / len(train_loader)\n",
    "        writer.add_scalar(\"Loss/Train\", avg_per_epoch_train_loss, epoch+1)\n",
    "\n",
    "        per_epoch_val_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in val_loader:\n",
    "            inputs, targets = data[0].to(device), data[1].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "                loss = criterion(outputs, targets)                \n",
    "                per_epoch_val_loss += loss.item()\n",
    "                \n",
    "        avg_per_epoch_val_loss = per_epoch_val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        writer.add_scalar(\"Loss/Val\", avg_per_epoch_val_loss, epoch+1)\n",
    "        writer.add_scalar(\"Accuracy/Val\", val_accuracy, epoch+1)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_per_epoch_train_loss:.4f}, \" \n",
    "              f\"Val Loss: {avg_per_epoch_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%, Time Elapsed {time.time() - time_per_epoch:.3f}s\")\n",
    "\n",
    "        if  avg_per_epoch_val_loss < best_val_loss:\n",
    "            patience_counter = 0\n",
    "            best_val_loss = avg_per_epoch_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"\\nTotal Time for Training {(end-start)/60:.3f}m\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c40e81b0-3d08-4dea-aee2-69216b18ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs  = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "aa2f9902-974d-4fd6-b6c7-7cc1e4354622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 1.9001, Val Loss: 1.7899, Val Acc: 33.84%, Time Elapsed 56.610s\n",
      "Epoch [2/5], Train Loss: 1.6878, Val Loss: 1.6607, Val Acc: 40.21%, Time Elapsed 57.438s\n",
      "Epoch [3/5], Train Loss: 1.5644, Val Loss: 1.5988, Val Acc: 41.87%, Time Elapsed 50.313s\n",
      "Epoch [4/5], Train Loss: 1.4805, Val Loss: 1.5559, Val Acc: 45.23%, Time Elapsed 54.452s\n",
      "Epoch [5/5], Train Loss: 1.4158, Val Loss: 1.5333, Val Acc: 45.58%, Time Elapsed 51.567s\n",
      "\n",
      "Total Time for Training 4.506m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "cifar10  = CIFAR10()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10.parameters(), lr=0.001)\n",
    "model = train(cifar10, train_loader, val_loader, criterion, optimizer, device=device, epochs=5)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "76d1c3b6-4aa1-40a4-8b61-546071cf4c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 48.54%\n"
     ]
    }
   ],
   "source": [
    "eval(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6b92185-8faf-41a1-90cd-4e294ebe91b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10V2(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10V2, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(nn.BatchNorm1d(output_neurons))\n",
    "        layers.append(Swish())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.BatchNorm1d(output_neurons))\n",
    "            layers.append(Swish())\n",
    "\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "949d346c-c56a-439a-82e7-edda2468d878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 2.0241, Val Loss: 1.8095, Val Acc: 33.79%, Time Elapsed 61.484s\n",
      "Epoch [2/5], Train Loss: 1.7113, Val Loss: 1.6768, Val Acc: 40.24%, Time Elapsed 61.726s\n",
      "Epoch [3/5], Train Loss: 1.6001, Val Loss: 1.6090, Val Acc: 43.22%, Time Elapsed 60.703s\n",
      "Epoch [4/5], Train Loss: 1.5270, Val Loss: 1.5700, Val Acc: 44.20%, Time Elapsed 62.025s\n",
      "Epoch [5/5], Train Loss: 1.4740, Val Loss: 1.5388, Val Acc: 45.88%, Time Elapsed 61.364s\n",
      "\n",
      "Total Time for Training 5.122m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "\n",
    "cifar10v2  = CIFAR10V2()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10v2.parameters(), lr=0.001)\n",
    "model = train(cifar10v2, train_loader, val_loader, criterion, optimizer, device=device, epochs=5)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1dc4c6e0-a5f4-4e10-85d0-b35c0a0beddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_lecun(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        gain = np.sqrt(sum(layer.weight.shape) / layer.weight.shape[1])\n",
    "        torch.nn.init.xavier_normal_(layer.weight, gain=gain)\n",
    "        torch.nn.init.constant_(layer.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9c815396-c5d2-452f-bb9b-4562d1926e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10V3(nn.Module):\n",
    "    def __init__(self, input_features=3*32*32, output_neurons=100, num_classes=10, hidden_layers=20):\n",
    "        super(CIFAR10V3, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layer_first = nn.Linear(input_features, output_neurons)\n",
    "        layers.append(layer_first)\n",
    "        layers.append(nn.SELU())\n",
    "        \n",
    "        for i in range(hidden_layers-1):\n",
    "            layer = nn.Linear(output_neurons, output_neurons)\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.SELU())\n",
    "\n",
    "        layer_last = nn.Linear(output_neurons, num_classes)\n",
    "        layers.append(layer_last)\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.net.apply(weights_init_lecun)\n",
    "\n",
    "    def forward(self, X):\n",
    "        flatten = nn.Flatten()\n",
    "        X = flatten(X)\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d78e1371-2be1-4646-802f-a5fc8c9cdf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 2.1656, Val Loss: 2.0423, Val Acc: 28.54%, Time Elapsed 22.917s\n",
      "Epoch [2/5], Train Loss: 1.7683, Val Loss: 2.1380, Val Acc: 30.68%, Time Elapsed 23.921s\n",
      "Epoch [3/5], Train Loss: 1.6571, Val Loss: 1.7635, Val Acc: 39.64%, Time Elapsed 26.930s\n",
      "Epoch [4/5], Train Loss: 1.5897, Val Loss: 2.0045, Val Acc: 32.44%, Time Elapsed 52.490s\n",
      "Epoch [5/5], Train Loss: 1.5328, Val Loss: 1.7638, Val Acc: 39.46%, Time Elapsed 52.903s\n",
      "\n",
      "Total Time for Training 2.988m\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "writer = SummaryWriter()\n",
    "standardize = transforms.Compose(\n",
    "     [transforms.Normalize((0, 0, 0), (1, 1, 1))]\n",
    ")\n",
    "\n",
    "cifar10v3  = CIFAR10V3()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.NAdam(cifar10v3.parameters(), lr=0.001)\n",
    "model = train(cifar10v3, train_loader, val_loader, criterion, optimizer, device=device, epochs=5)\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a83ee331-06ef-40c5-8329-01edf5eb53b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 20852), started 1 day, 17:38:20 ago. (Use '!kill 20852' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8f65e2bd18d09b9f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8f65e2bd18d09b9f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a4903-62a6-45bc-b98c-2dbe456eedb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
